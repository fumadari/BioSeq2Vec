model:
  vocab_size: 5  # A, C, G, T, N
  embedding_dim: 128
  nhead: 8
  num_layers: 4
  output_dim: 64
training:
  epochs: 10
  batch_size: 32
  learning_rate: 0.001
  device: 'cuda'  # 'cuda' or 'cpu'
data:
  sequences_file: 'data/sample_sequences.fasta'
